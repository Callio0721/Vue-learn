# **NavGPT：视觉 - 语言导航中的显性推理**

## 一、文章大致内容介绍

这篇论文的核心是**让大语言模型（比如 GPT-4、GPT-3.5）直接当 “导航机器人”**—— 不用专门训练，就能听懂人类的自然语言指令（比如 “穿过厨房，从黑色大书柜右边的门出去，下楼梯到第三级停下”），在陌生环境里完成导航。

NavGPT 每一步均以视觉观测的文本描述、导航历史和未来可探索方向为输入，推理自身当前状态并决策向目标靠近。其核心是让 LLM 显式执行高层导航规划，无需针对 VLN 任务额外训练。

## 二、VLN 任务的研究现状与不足

视觉 - 语言导航（VLN）是实现现实世界指令跟随具身智能体的探索性任务，现有研究尝试利用 GPT 模型辅助导航：如将 LLM 作为语言输入解析器提取地标，或利用其常识推理能力整合物体间关联知识。但 LLM 在导航中的推理能力仍未被充分探索 ——LLM 能否理解文本形式的交互世界、动作及后果，并利用这些信息解决导航任务，仍是待解问题

## 三、方法

### 3.1  VLN 问题定义（VLN Problem Formulation）

**1. 自然语言指令**

自然语言指令由一系列离散单词构成，形式化定义为：
$$w = \{w_1, w_2, ..., w_n\}$$
其中：
- $n$ 为指令中单词的总数；
- $w_i$ 表示指令中的第 $i$ 个单词（$1 \leq i \leq n$）。

**2. 观测（Observation）**

在每一时间步 $t$，智能体通过模拟器获取观测集合 $O_t$，该集合包含 $N$ 个可选的自我中心视角（对应智能体不同朝向），每个视角关联唯一的角度方向。

**形式化定义**：$$O_t \triangleq [<o_1,a_1>, <o_2,a_2>, ..., <o_N,a_N>]$$

| 符号 | 含义 |
|------|------|
| $o_i$ | 第 $i$ 个可选视角的视觉/环境观测数据 |
| $a_i$ | 第 $i$ 个视角对应的角度方向 |
| $N$ | 每一步的可选视角总数 |
| $t$ | 时间步索引（离散时间） |

**3. 动作空间（Action Space）**

智能体的动作被约束在预定义的导航图 $G$ 中，需从下一时刻的候选视角集合 $C_{t+1}$ 中选择动作。

候选视角集合对应的观测表示为：
$$O_t^C \triangleq [<o_1^C,a_1^C>, ..., <o_M^C,a_M^C>]$$

即能移动到的下一个 viewpoint 对应的环境描述，比如 “下一个能去的位置有 3 个，分别对应厨房门口、客厅中间、过道拐角的景象

| 符号 | 含义 |
|------|------|
| $C_{t+1}$ | $t+1$ 时刻的候选视角集合 |
| $M$ | 候选视角集合的规模，即 $M=|C_{t+1}|$ |
| $o_i^C$ | 第 $i$ 个候选视角的观测数据 |
| $a_i^C$ | 第 $i$ 个候选视角对应的相对角度（动作核心参数） |
| $G$ | 智能体导航的约束图（如拓扑图/栅格图） |

**4. 状态转换（State Transition）**

智能体通过选择候选视角的相对角度 $a_i^C$ 执行动作，实现从当前状态到下一状态的转换。

- 当前状态：$s_t = <v_t, \theta_t, \phi_t>$
- 下一状态：$s_{t+1}$（结构与 $s_t$ 一致）

| 维度 | 符号 | 含义 |
|------|------|------|
| 位置 | $v_t$ | 智能体当前视角的空间位置 |
| 航向角 | $\theta_t$ | 智能体水平方向的朝向角度 |
| 仰角 | $\phi_t$ | 智能体垂直方向的朝向角度 |

**5. 历史记录与条件转移概率**

智能体维护状态历史序列，并基于历史动态调整状态转移的条件概率分布。

- 状态历史：$h_t$（包含 $t$ 及之前所有时刻的状态、动作、观测信息）
- 条件转移概率：
  $$S_t = T(s_{t+1} | a_i^C, s_t, h_t)$$

| 符号 | 含义 |
|------|------|
| $T(\cdot|\cdot)$ | 条件概率分布函数 |
| $S_t$ | $t$ 时刻的状态转移概率分布 |
| $a_i^C$ | 执行的动作（候选视角的相对角度） |
| $s_t$ | 执行动作前的当前状态 |
| $h_t$ | 执行动作前的状态历史 |

**6. 策略定义**

智能体需学习的策略 π 由参数 Θ 表征，基于指令 w 和当前候选观测$O_t^C$，即$\pi(a_t | W, O_t, O_t^C, S_t ; \Theta)$”

这句话是在定义 “导航策略的完整逻辑”，结合公式通俗说：

- 公式里的 “|” 表示 “基于这些输入”，“;” 表示 “由这个参数控制”；
- 输入有 4 个关键信息：
  - W：人类的导航指令（比如 “穿过厨房，从黑色书柜右边的门出去”）；
  - $O_t$：智能体当前看到的所有环境观测（比如 “前方是客厅、左边是厨房、右边是过道”）；
  - $O_t^C$：当前可选择的 “候选观测”（即能移动到的下一个 viewpoint 对应的环境描述，比如 “下一个能去的位置有 3 个，分别对应厨房门口、客厅中间、过道拐角的景象”）；
  - $S_t$：当前状态的转移概率（比如 “从当前位置走到厨房门口的概率是 100%，走到客厅中间的概率是 100%”，本质是 “哪些动作是可行的”）；
- 输出$a_t$：智能体下一步的动作（即选择哪个候选 viewpoint，比如 “选厨房门口的 viewpoint ID”）；
- 核心逻辑：策略 π 在参数 Θ 的控制下，结合 “指令、当前环境、可选路径、可行动作”，算出下一步该做什么。NavGPT 以零样本方式执行 VLN 任务：$\Theta$并非从 VLN 数据集学习，而是来自 LLM 训练的语言语料库

### 3.2 NavGPT 的核心架构与模块

![Snipaste_2025-12-27_15-27-53](images\Snipaste_2025-12-27_15-27-53.png)

NavGPT 是一个协同环境、语言引导、导航历史进行动作预测的系统，其核心逻辑通过公式（1）定义：

$$A_{t+1} = \text{LLM}\big(\text{M}(\text{P}),\text{M}(\text{W}),\text{M}(\text{F}(O_{t})),\text{M}(H_{<<t+1})\big)$$

其中：
$A_{t+1}$：智能体下一步动作；
$\text{LLM}$：核心推理模块（如 GPT-4、GPT-3.5）；
$\text{M}$：提示管理器（Prompt Manager），负责将各类输入转换为 LLM 可理解的自然语言；
输入要素包括 4 类：导航系统规则$\text{P}$、语言指令$\text{W}$、视觉观测经 VFMs 处理后的结果$\text{F}(O_{t})$、导航历史$H_{<<t+1}$。

公式的意思：**LLM（大脑）接收 “规则 + 指令 + 当前环境描述 + 历史记录”，经过思考后，输出下一步的导航动作**。

以下详细解释各核心模块：
#### 3.2.1 导航系统规则（Navigation System Principle, $\text{P}$）
定义 LLM 作为 VLN 智能体的行为准则，明确 VLN 任务定义、每一步导航的基本推理格式和规则。例如：NavGPT 需在环境预定义图的静态视角（位置）间移动，需识别唯一视角 ID，不得编造不存在的 ID。

#### 3.2.2 视觉基础模型（Visual Foundation Models, $\text{F}$）
NavGPT 作为 LLM 智能体，缺乏直接视觉感知能力，需 VFMs 提供视觉感知和表达能力 —— 将当前环境的视觉观测转换为自然语言描述。VFMs 扮演 “翻译者” 角色，可用自然语言、物体边界框、物体深度等 “语言” 翻译视觉观测，经提示管理器处理后，转化为纯自然语言供 LLM 理解。

#### 3.2.3 导航历史（Navigation History, $H_{<<t+1}$）
对 NavGPT 评估指令完成进度、更新当前状态、制定后续决策至关重要。历史由前$t$步的 “观测$O$- 推理$R$- 动作$A$” 三元组组成，即$H_{<<t+1} \triangleq [<O_1,R_1,A_1>,<O_2,R_2,A_2>,...,<O_t,R_t,A_t>]$，包含 LLM 的推理思考过程。

#### 3.2.4 提示管理器（Prompt Manager, $\text{M}$）
将导航系统规则、语言指令、VFMs 输出、导航历史转换为 LLM 可理解的自然语言，是 LLM 作为 VLN 智能体的关键。其核心功能是收集各组件结果，解析为单一提示，供 LLM 制定导航决策。

### 3.3 NavGPT 的视觉感知过程（Visual Perceptron for NavGPT）

视觉信号对 LLM 而言如同 “外语”，需通过不同 VFMs 转换为自然语言，具体流程如下：

#### 3.3.1 多视角观测采集

智能体在每个视角可获取 24 个自我中心视角图像：水平方向（航向角$\theta$）从 0° 到 360°，每 45° 一个视角（共 8 个）；垂直方向（仰角$\phi$）从水平线上 30° 到水平线下方 30°，每 30° 一个视角（共 3 个），总计\(3×8=24\)个视角。

#### 3.3.2 视觉观测的文本转换

![Snipaste_2025-12-27_15-33-08](images\Snipaste_2025-12-27_15-33-08.png)

1. 场景描述生成：使用 BLIP-2 模型作为翻译器，其强大的零样本图像到文本生成能力，可详细描述每个视角的物体形状、颜色及场景，同时避免小视场角（FoV）下的无效描述；
2. 视角信息整合：航向角方向的视角无重叠，垂直方向的上、中、下视角有 15° 重叠。NavGPT 聚焦航向角导航，因此用 GPT-3.5 将每个航向角对应的 3 个垂直视角场景总结为一句话描述；
3. 辅助视觉特征提取：除 BLIP-2 的场景描述外，还利用 Fast-RCNN 提取物体边界框，计算物体与智能体的相对航向角，并从 Matterport3D 模拟器提取物体中心像素的深度信息；
4. 有效信息筛选与整合：筛选距离当前视角 3 米内的物体，将物体类别、相对方向、深度等信息经提示管理器处理，转化为当前视角的自然语言观测描述。

### 3.4 LLM 中推理与动作的协同（Synergizing Reasoning and Actions in LLMs）

#### 3.4.1 动作空间扩展

VLN 任务中，智能体需学习的策略$\pi(a_t | W, O_t, O_t^C, S_t ; \Theta)$面临动作与观测间隐含关联复杂、计算量大的问题。为增强智能体对当前状态的理解，本文借鉴 ReAct 论文，将智能体的动作空间扩展为$\tilde{A} = A \cup R$：其中A为原始动作空间，$R \in L$（L为整个语言空间）表示智能体的思考或推理轨迹。

#### 3.4.2 推理轨迹的作用

推理轨迹R不会触发外部环境交互，输出推理时无观测返回。让 LLM 先输出推理轨迹再做导航决策，主要有两大作用：

1. 促进复杂推理：让 LLM 在选择动作前思考，可制定长期导航计划（如指令分解、子目标规划），这是现有工作中未明确实现的；
2. 提升决策连贯性：将推理轨迹纳入导航历史，使 NavGPT 能继承先前推理，持续推进子目标达成，同时跟踪导航进度，具备计划调整的异常处理能力。

### 3.5 NavGPT 提示管理器的具体实现

提示管理器将导航系统规则、VFMs 翻译结果、导航历史解析为 LLM 的输入提示，具体处理方式：

1. 导航系统规则：生成提示传递 LLM 行为规则，明确 VLN 任务定义、模拟环境信息、推理格式约束；
2. VFMs 感知结果：按当前 NavGPT 朝向为前方，将 8 个航向角的描述按顺时针顺序拼接，整合为统一的观测提示；
3. 导航历史：“观测 - 推理 - 动作” 三元组存储于历史缓冲区，直接输入会因长度超限导致 LLM 无法处理。因此用 GPT-3.5 总结轨迹中的观测信息，将总结结果插入三元组，形成精简的历史提示。

## 

