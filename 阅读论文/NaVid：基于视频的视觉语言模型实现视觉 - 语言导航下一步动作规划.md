# NaVid：基于视频的视觉语言模型实现视觉 - 语言导航下一步动作规划

## 一、文章大致介绍

这篇文章核心是研发了一个叫 **NaVid** 的 “机器人导航系统”—— 它能像人一样，只靠摄像头拍的视频和人类说的指令，在陌生环境里自主导航，不用地图、不用测距仪，也不用深度传感器，解决了机器人从模拟环境走进真实世界、适应不同场景的大难题。

- 现在的大模型（比如 ChatGPT、能看图片的 GPT-4V）在很多领域都很厉害，比如写文章、自动驾驶、机器人操作，泛化能力超强 —— 能适应没见过的场景。
- 论文的核心想法：能不能用大模型的优势，解决 VLN 的泛化问题？于是就有了 NaVid

## 二、目前技术难点

我们先想一个场景：让机器人帮你 “走进客厅，找到白色的垃圾桶旁边停下”。这个任务看着简单，但对机器人来说超难，主要难在 3 点：

1. **“看不懂” 环境**：机器人不知道哪是客厅、哪是垃圾桶，也没法判断自己离目标多远；
2. **“听不懂” 指令**：人类的指令是自然语言（比如 “旁边停下”），不是机器人能直接执行的代码；
3. **“适应不了” 新环境**：在实验室（模拟环境）里训练好的机器人，到你家（真实环境）可能就 “迷路”—— 因为光线、家具摆放都不一样，这就是 “从模拟到真实” 的鸿沟；甚至换个房间（不同场景）也会失灵。

之前的机器人导航系统，要么需要提前画好地图、装测距仪（成本高、容易坏），要么只能在固定场景用（比如只认识实验室的垃圾桶），没法通用。这篇文章就是要做一个 “接地气” 的系统：不用复杂设备，只靠普通摄像头 + 人类指令，就能在各种陌生环境里导航。

## 三、目前的相关工作

### 1. 视觉 - 语言导航（VLN）

- 早期：都在 “跳格子” 的离散环境里研究，机器人靠匹配语言和视觉信息，在节点间移动。虽然效率高，但没法直接用到真实机器人上 —— 真实机器人不能跳着走。
- 后来：出现了连续环境的 VLN（VLN-CE），机器人能自由移动，要么预测具体动作（比如走多少、转多少），要么先找 “中间目标”（比如先走到门口，再走到桌子）。同时，很多方法开始用大模型的视觉 - 语言能力，让机器人更懂指令和环境，但还是依赖额外设备。

### 2. VLN 的模拟到真实迁移

- 之前的问题：大多数方法只在模拟环境里测试，忽略了真实环境的复杂情况（比如光线变化、物体遮挡）。有研究发现，模拟环境里的成功率到真实环境会掉 50% 以上。
- 现在的趋势：用大模型的泛化能力，让机器人能适应真实环境和不同风格的指令 ——NaVid 就是顺着这个趋势，而且做得更彻底（不用额外设备）。

### 3. 大模型作为具身智能体

- 之前的应用：大模型已经用于机器人操作（比如控制机械臂拿东西）、自动驾驶、游戏里的角色控制等，但很少用于连续环境的 VLN。
- NaVid 的突破：第一个把视频基大模型用于连续环境的 VLN，让机器人只靠 “看视频 + 听指令” 就能导航，填补了这个空白。

## 四、问题定义

NaVid 要完成什么任务：

- 场景：连续环境（机器人能自由移动，不是跳格子）。
- 输入：① 人类的自然语言指令（比如 “走进实验室，在白色柜子前停下”）；② 机器人摄像头从出发到现在拍的所有视频（从第 0 帧到第 t 帧）。

- 输出：机器人的下一步 “具体动作”（比如 “向前走 50 厘米”“右转 45 度”“停下”）。
- 循环逻辑：机器人执行完这个动作，会拍到新的画面（第 t+1 帧），再结合原来的指令，输出下一个动作，直到完成指令。
- 即：在时间 t，给定一个包含 l 个单词的自然语言指令 I 和一个由一系列帧 {x0,・・・, xt} 组成的视频观测 Ot，智能体需要为下一步规划一个低级动作 at+1 ∈ A
- 关键设定：
  - 观测：只靠单目 RGB 相机（普通摄像头），没有其他数据；
  - 动作：分 4 种类型（向前、左转、右转、停下），每种都有具体参数（向前 = 距离，左转 / 右转 = 角度），和人类导航的逻辑一致。

## 五、NaVid 的核心设计（怎么让机器人 “看视频、懂指令、做动作”）

<<<<<<< HEAD
![Snipaste_2025-12-24_18-20-24](E:\Personal-Learns\Vue\Vue-learn\阅读论文\image\Snipaste_2025-12-24_18-20-24.png)
=======
![Snipaste_2025-12-24_18-20-24](images\Snipaste_2025-12-24_18-20-24.png)
>>>>>>> d0241d99d10d149210e79d588bfdfbcadc85e35a

### 1. 整体架构

- 基础：基于 LLaMA-VID 的视频基大模型
- 组件：4 个核心部分，像人的 “眼睛、大脑、转换器”：
  - 视觉编码器：把视频画面翻译成机器人能懂的 “信息块”（叫 “视觉令牌”），使用比如把 “白色柜子” 的画面变成对应的信息；
  - 查询生成器：从视觉令牌里，专门挑出和指令相关的信息（比如指令是 “找白色柜子”，就挑出 “白色、柜状” 的信息）；
  - 跨模态投影器：把视觉令牌和语言令牌（指令翻译的信息块）“转换成同一种语言”，让机器人能同时处理；
  - LLM（大语言模型）：核心决策器，结合所有信息，输出下一步动作。

### 2. 观测编码（“看视频”：从画面里抓有用信息）

NaVid 看视频每帧画面会提取 2 类信息，还会区分当前和历史画面：

- 2 类视觉令牌：
  1. 指令相关令牌：只抓和指令匹配的视觉特征（比如指令 “找白色垃圾桶”，就忽略墙上的画、地上的电线，只盯 “白色、桶状” 的物体）；
  2. 通用环境令牌：抓画面里的通用信息（比如墙壁、门、家具形状），帮机器人判断 “能往哪走”（比如看到墙壁就知道不能往前走）、“自己在哪”（比如看到门就知道是房间入口）。
- 历史 vs 当前画面：
  - 当前画面（正在拍的）：提取 64 个通用令牌（看得细，不遗漏关键细节）；
  - 历史画面（之前拍的）：每个帧只提取 4 个通用令牌（记关键信息就行，比如 “走过了一个门”，不用记门的颜色、纹理，节省 “脑力”）。
- 特殊令牌：给不同信息贴 “标签”，避免机器人混淆：
  - < HIS > 和 < /HIS >：包裹历史画面的信息（比如 “<HIS> 走过一个门 </HIS>”）；
  - < OBS > 和 < /OBS >：包裹当前画面的信息（比如 “<OBS> 前方有白色桶 </OBS>”）；
  - < NAV >：提醒机器人“现在要开始做导航决策了”（比如“<NAV> 找白色垃圾桶停下 </NAV>”）。

### 3. 动作规划（“做决策”：输出具体动作）

- 输出格式：用自然语言描述动作（比如 “向前走 75 厘米”“左转 30 度”），包含 2 部分：
  1. 动作类型：只能是 4 种（向前、左转、右转、停下）；
  2. 具体参数：向前 = 距离（厘米），左转 / 右转 = 角度（度），停下没有参数。
- 执行：用 “正则表达式解析”（不用懂技术）把自然语言动作转换成机器人能直接执行的指令（比如把 “向前走 75 厘米” 变成机器人的电机控制信号）。

### 4. 训练策略（“教 NaVid 导航”：怎么让它学会）

NaVid 不是天生会导航，是靠 “海量练习 + 聪明方法” 学会的：

- 核心数据：51 万条导航样本，分 2 类：
  1. Oracle 轨迹（32 万条）：“正确路线”—— 机器人在模拟环境里按最优路径完成指令，记录 “视频 + 指令 + 正确动作”（比如看到白色垃圾桶，正确动作是 “停下”）；
  2. 非 Oracle 轨迹（18 万条）：“错误路线 + 修正”—— 用一种叫 Dagger 的技术，让机器人先按学过的方法走，走错了就记录 “错误动作 + 正确修正”（比如走偏了，记录 “左转 10 度修正”），这样机器人遇到类似情况就不会再错，更鲁棒。
- 辅助任务（帮机器人 “举一反三”）：
  1. 指令推理（1 万条）：给机器人一段导航视频，让它猜 “人类当时给的指令是什么”（比如视频里机器人走到垃圾桶前停下，让它猜指令是 “找白色垃圾桶停下”），帮它理解 “动作和指令的关系”；
  2. 视频问答：给视频提问题（比如 “画面里有门吗？”“桌子在左边还是右边？”），让机器人回答，帮它抓关键环境信息。
- 训练细节：
  - 硬件：24 个 NVIDIA A100 显卡，训练了 28 小时；
  - 基础：加载了 EVA-CLIP（会看图片）、BERT（会懂语言）、Vicuna-7B（会做决策）的预训练权重 —— 相当于站在巨人的肩膀上，不用从零学，只优化导航相关的能力。

## 六、实验

### 1. 实验设置

- 测试环境：
  1. 模拟环境：用 VLN-CE 的 R2R 和 RxR 数据集（都是室内场景的导航任务），测试 “陌生场景泛化”和 “跨数据集泛化”（训练用 R2R，测试用 RxR）；
  2. 真实环境：4 个不同室内场景（会议室、办公室、实验室、休息室），2 种指令（简单指令：比如 “走到门口停下”；复杂指令：比如 “先左转，再走到蓝色桌子前停下”），共 200 条指令。
- 测试机器人：Turtlebot4，装了 Kinect DK 摄像头（拍 RGB 视频）和激光雷达（给其他基线方法用，NaVid 不用）。
- 评价指标：
  - 成功率（SR）：完成指令的比例（比如 100 条指令完成 37 条，成功率 37%）；
  - SPL：又准又高效（不仅能完成指令，走的路还短，不绕远）；
  - NE：导航误差（离目标的距离，越小越好）；
  - 关键规则：模拟环境里，机器人停下时离目标≤3 米算成功；真实环境里≤1.5 米算成功。
- 对比对象（基线）：之前的主流方法，比如 Seq2Seq（靠循环神经网络预测动作）、CMA（靠跨模态注意力）、WS-MGMap（靠多粒度地图），还有大模型（GPT-4V、LLaVA、LLaMA-VID）。

### 2. 模拟环境实验结果

- 核心结论：NaVid 只用 RGB 视频，却比依赖深度、测距仪、地图的方法表现更好：
  1. R2R 数据集：SPL（35.9%）、NE（5.47 米）都是顶尖水平，比同样只用 RGB 的方法（RGB-CMA 的 SPL 只有 4.43%）强太多；
  2. RxR 数据集（跨数据集测试）：成功率 23.8%，SPL21.2%，比当前最好的零样本方法（A²Nav）成功率高 41.7%，SPL 高 236.5%—— 说明 NaVid 能适应没见过的指令和场景；
  3. 和大模型对比：GPT-4V 成功率只有 5%，LLaVA、LLaMA-VID 几乎输出不了有效动作（净说场景描述），而 NaVid 经过导航数据训练后，成功率 38%，有效动作输出比例 91.3%—— 说明专门的导航训练很重要；
  4. 历史轨迹表示对比：比用文本、地图 + 文本、第一人称视图 + 文本的方法好太多（SPL 从 35.9% 降到 20.8% 以下），而且视频基的推理速度更快（1.2 秒 / 步 vs 文本基的 2.7 秒 / 步）—— 说明视频能保留更全的信息，不压缩关键细节。

### 3. 真实环境实验结果

- 核心结论：在真实场景里，NaVid 的表现远超其他方法：
  1. 成功率：简单指令平均 84%（比如 “走到门口停下”），复杂指令平均 48%（比如 “先左转，再走到蓝色桌子前停下”）；
  2. 对比基线：Seq2Seq、CMA 的成功率几乎为 0（模拟到真实鸿沟太大），WS-MGMap（靠地图）的简单指令成功率约 50%，复杂约 20%，NaVid 全面领先；
  3. 实战案例：机器人面对 “同类物体”（比如指令是 “走到右边的椅子”，但机器人一开始面对左边的椅子），能准确转弯找到目标 —— 说明 NaVid 能理解指令的空间关系，不认错目标。

### 4. 消融实验

- 去掉协同训练：SPL 从 35.9% 降到 23.6%—— 说明多任务训练（导航 + 指令推理 + 视频问答）能提升泛化能力；
- 去掉指令推理样本：SPL 降到 29.1%—— 说明 “看视频猜指令” 能帮模型懂环境和指令的关系；
- 去掉非 Oracle 轨迹样本：SPL 降到 32.0%—— 说明 “错误路线 + 修正” 能让模型更鲁棒；
- 去掉特殊令牌（<HIS> <OBS> <NAV>）：SPL 降到 33.4%—— 说明这些标签能帮模型区分信息，不混淆；
- 视觉令牌数量：1 个令牌（SPL20.5%）→4 个（35.9%）→16 个（36.1%）——4 个令牌是 “性价比最高”，多了速度变慢（1.22 秒→2.72 秒），性能提升却很少（0.2%）。

### 5. 数据规模实验

- 预训练阶段（0-330k 数据）：数据少于 280k 时，性能提升很慢；达到 330k 时，性能突然暴涨 —— 说明模型开始真正掌握导航技能；
- 后训练阶段（330k-510k 数据）：性能提升很少 —— 说明现有数据的环境和指令不够多样，再多数据也难提升。
